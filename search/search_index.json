{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p><code>video_features</code> allows you to extract features from video clips. It supports a variety of extractors and modalities, i. e. visual appearance, optical flow, and audio.</p>"},{"location":"#supported-models","title":"Supported Models","text":"<p>Action Recognition</p> <ul> <li>S3D (Kinetics 400)</li> <li>R(2+1)d RGB (IG-65M, Kinetics 400)</li> <li>I3D-Net RGB + Flow (Kinetics 400)</li> </ul> <p>Sound Recognition</p> <ul> <li>VGGish (AudioSet)</li> </ul> <p>Optical Flow</p> <ul> <li>RAFT (FlyingChairs, FlyingThings3D, Sintel, KITTI)</li> </ul> <p>Frame-wise Features</p> <ul> <li>All models from TIMM e.g. ViT, ConvNeXt, EVA, Swin, DINO (ImageNet, LAION, etc)</li> <li>CLIP</li> <li>ResNet-18,34,50,101,152 (ImageNet)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># clone the repo and change the working directory\ngit clone https://github.com/v-iashin/video_features.git\ncd video_features\n\n# install environment\nconda env create -f conda_env.yml\n\n# load the environment\nconda activate video_features\n\n# extract r(2+1)d features for the sample videos\npython main.py \\\n    feature_type=r21d \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n# if you have many GPUs, just run this command from another terminal with another device\n# device can also be \"cpu\"\n</code></pre> <p>If you are more comfortable with Docker, there is a Docker image with a pre-installed environment that supports all models. Check out the Docker support. documentation page.</p>"},{"location":"#multi-gpu-and-multi-node-setups","title":"Multi-GPU and Multi-Node Setups","text":"<p>With <code>video_features</code>, it is easy to parallelize feature extraction among many GPUs. It is enough to start the script in another terminal with another GPU (or even the same one) pointing to the same output folder and input video paths. The script will check if the features already exist and skip them. It will also try to load the feature file to check if it is corrupted (i.e. not openable). This approach allows you to continue feature extraction if the previous script failed for some reason.</p> <p>If you have an access to a GPU cluster with shared disk space you may scale extraction with as many GPUs as you can by creating several single-GPU jobs with the same command.</p> <p>Since each time the script is run the list of input files is shuffled, you don't need to worry that workers will be processing the same video. On a rare occasion when the collision happens, the script will rewrite previously extracted features.</p>"},{"location":"#used-in","title":"Used in","text":"<ul> <li>SpecVQGAN branch <code>specvqgan</code></li> <li>BMT branch <code>bmt</code></li> <li>MDVC branch <code>mdvc</code></li> </ul> <p>Please, let me know if you found this repo useful for your projects or papers.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<ul> <li>@Kamino666: added CLIP model as well as Windows and CPU support (and many other useful things).</li> <li>@ohjho: added support of 37-layer R(2+1)d favors.</li> <li>@borijang: for solving bugs with file names, I3D checkpoint loading enhancement and code style improvements.</li> <li>@bjuncek: for helping with timm models and offline discussion.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful in your research, please consider cite:</p> <pre><code>@misc{videofeatures2020,\n  title = {Video Features},\n  author = {Vladimir Iashin and other contributors},\n  year = {2020},\n  howpublished = {\\url{https://github.com/v-iashin/video_features}},\n}\n</code></pre>"},{"location":"meta/docker/","title":"Docker support","text":"<p>For your convenience, there is also a Docker image with the pre-installed environments that supports all models. The Docker image does not have the <code>video_features</code> library inside which allows you to tweak the code locally, mount the new version, and just use the container as an environment. It is assumed that you have Docker and nvidia-container-runtime installed.</p>"},{"location":"meta/docker/#setup","title":"Setup","text":"<p>Start by cloning the repo locally if you haven't done it already <pre><code>git clone https://github.com/v-iashin/video_features.git\n</code></pre></p> <p>Download the docker image or build it yourself: <pre><code>docker pull iashin/video_features\n# preventing newer versions of the image to be downloaded unexpectedly\ndocker tag iashin/video_features video_features\n# or\n# cd video_features\n# docker build --tag video_features .\n</code></pre></p> <p>Once it is done, mount (<code>--mount</code>) the cloned repository folder, and initialize a container with the 0th GPU but remember: just like with any mount, a change from inside of the container will be reflected in the mounted folder (<code>/absolute/path/to/video_features/</code>).: <pre><code>docker run -it \\\n    --mount type=bind,source=\"/absolute/path/to/video_features/\",destination=\"/home/ubuntu/video_features/\" \\\n    --shm-size 8G \\\n    -it --gpus '\"device=0\"' \\\n    video_features:latest \\\n    bash\n# and you should get the bash shell:\n# ubuntu@56b1bf77a20c:~$\n</code></pre></p> <p>Check if a GPU is available to PyTorch: <pre><code># ubuntu@56b1bf77a20c:~$\npython -c \"import torch; print(torch.cuda.is_available())\"\n# True\n</code></pre></p> <p>Finally, try to extract video features: <pre><code># cd to `./video_features`\n# ubuntu@56b1bf77a20c:~/video_features $\npython main.py \\\n    feature_type=r21d \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p>"},{"location":"meta/docker/#extract-features-from-custom-videos","title":"Extract features from custom videos","text":"<p>You need to mount the folders with video files before you start the container.</p> <p>If the folder with custom videos is already in <code>./video_features</code>, you don't have to do anything. Any changes from inside of the container will be reflected in your original dataset (use a backup!). Here is an example of mounting a folder from somewhere else (mounts <code>/absolute/path/somewhere/else/</code> to <code>/home/ubuntu/video_features/dataset</code>): <pre><code>docker run -it \\\n    --mount type=bind,source=\"/absolute/path/to/video_features/\",destination=\"/home/ubuntu/video_features/\" \\\n    --mount type=bind,source=\"/absolute/path/somewhere/else/\",destination=\"/home/ubuntu/video_features/dataset/\" \\\n    --shm-size 8G \\\n    -it --gpus '\"device=0\"' \\\n    video_features:latest \\\n    bash\n# ubuntu@56b1bf77a20c:~$\nls ./video_features\n# ... dataset ...\n</code></pre></p> <p>If you want to save outputs to another folder on your local machine, you may want to mount it as well: e.g. by adding <pre><code>...\n    --mount type=bind,source=\"/absolute/path/somewhere/else/\",destination=\"/home/ubuntu/video_features/output/\" \\\n...\n</code></pre></p> <p>Then, run your command. For instance: <pre><code># cd to `./video_features`\n# ubuntu@56b1bf77a20c:~/video_features $\npython main.py \\\n    feature_type=r21d \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./dataset/vid_1.mp4, ./dataset/vid_2.mp4]\" \\\n    on_extraction=\"save_numpy\"\n# you should have features in `./output`\n# (and in the source location if you mount to it)\n</code></pre></p>"},{"location":"meta/docker/#switching-conda-environments","title":"Switching conda environments","text":"<p>By default, the <code>video_features</code> environment is activated once you attach the shell. The image supports both conda environments and you can switch it simply as follows: <pre><code># ubuntu@56b1bf77a20c:~$\nconda activate video_features\n# which python\n</code></pre></p>"},{"location":"meta/install_conda/","title":"Setup Environment","text":""},{"location":"meta/install_conda/#using-the-yaml-file","title":"Using the YAML File","text":"<p>If you want to quickly set up the <code>conda</code> environment with all the required dependencies, use the <code>conda_env.yml</code> file. Run the following command:</p> <pre><code># it will create a new conda environment called 'video_features' on your machine\nconda env create -f conda_env.yml\n</code></pre>"},{"location":"meta/install_conda/#from-scratch","title":"From Scratch","text":"<p>Just steps to install conda and create a new environment from scratch.</p> <pre><code>conda create -n video_features\nconda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\nconda install -c conda-forge omegaconf scipy tqdm pytest opencv\n# +CLIP\nconda install -c conda-forge ftfy regex\n# vggish\nconda install -c conda-forge resampy pysoundfile\n# timm models\npip install timm\n</code></pre>"},{"location":"models/clip/","title":"CLIP","text":"<p>The CLIP features are extracted at each frame of the provided video. CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. We use CLIP's official augmentations and extract vision features from its image encoder. The implementation uses the OpenAI CLIP. The extracted features are going to be of size <code>num_frames x 512</code>. We additionally output timesteps in ms for each feature and fps of the video.</p>"},{"location":"models/clip/#quick-start","title":"Quick Start","text":"<p>Ensure that the environment is properly set up before proceeding. See Setup Environment for detailed instructions.</p> <p>Activate the environment <pre><code>conda activate video_features\n</code></pre></p> <p>and extract features at 1 fps from <code>./sample/v_GGSY1Qvo990.mp4</code> video and output results. <pre><code>python main.py \\\n    feature_type=\"clip\" \\\n    model_name=\"ViT-B/32\" \\\n    extraction_fps=1 \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\\n    on_extraction=\"print\"\n</code></pre></p>"},{"location":"models/clip/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>model_name</code> <code>\"ViT-B/32\"</code> A variant of CLIP. <code>\"ViT-B/16\"</code>, <code>\"RN50x16\"</code>, <code>\"RN50x4\"</code>, <code>\"RN101\"</code>, <code>\"RN50\"</code>, and <code>\"custom\"</code> are supported. <code>batch_size</code> <code>1</code> You may speed up extraction of features by increasing the batch size as much as your GPU permits. <code>extraction_fps</code> <code>null</code> If specified (e.g. as <code>5</code>), the video will be re-encoded to the <code>extraction_fps</code> fps. Leave unspecified or <code>null</code> to skip re-encoding. <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos). <code>show_pred</code> <code>false</code> If <code>true</code>, the script will print the predictions of the model on a down-stream task. It is useful for debugging. <code>pred_texts</code> <code>null</code> If <code>show_pred=true</code>, the texts specified in <code>pred_texts</code> are used for zero-shot classification (e.g. <code>pred_texts=\"['a dog smiles', 'a woman is lifting']\"</code>). If <code>pred_texts</code> is unspecified, Kinetics 400 classes will be used."},{"location":"models/clip/#examples","title":"Examples","text":"<p>Make sure the environment is set up correctly. For instructions, refer to Setup Environment.</p> <p>Start by activating the environment <pre><code>conda activate video_features\n</code></pre></p> <p>It is pretty much the same procedure as with other features. Here we take <code>ViT/B-32</code> as an example, but we also support <code>ViT-B/16</code>, <code>RN50x16</code>, <code>RN50x4</code>, <code>RN101</code>, <code>RN50</code> and others in OpenAI CLIP implementation. In addition, if you want to use your weights, you need to copy your weights to <code>models/clip/checkpoints</code>, rename it <code>CLIP-custom.pth</code> and specify <code>model_name=custom</code>. <pre><code>python main.py \\\n    feature_type=\"clip\" \\\n    model_name=\"ViT-B/32\" \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> If you would like to save the features, use <code>--on_extraction save_numpy</code> (or <code>save_pickle</code>) \u2013 by default, the features are saved in <code>./output/</code> or where <code>--output_path</code> specifies. In the case of frame-wise features, besides features, it also saves timestamps in ms and the original fps of the video into the same folder with features. <pre><code>python main.py \\\n    feature_type=\"clip\" \\\n    model_name=\"ViT-B/32\" \\\n    device=\"cuda:0\" \\\n    on_extraction=save_numpy \\\n    file_with_video_paths=./sample/sample_video_paths.txt\n</code></pre> We may increase the extraction speed with batching. Therefore, frame-wise features have <code>--batch_size</code> argument, which defaults to <code>1</code>. <pre><code>python main.py \\\n    feature_type=\"clip\" \\\n    model_name=\"ViT-B/32\" \\\n    device=\"cuda:0\" \\\n    batch_size=128 \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> If you would like to extract features at a certain fps, add <code>--extraction_fps</code> argument <pre><code>python main.py \\\n    feature_type=\"clip\" \\\n    model_name=\"ViT-B/32\" \\\n    device=\"cuda:0\" \\\n    extraction_fps=5 \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p> <p>If you would like to verify the extracted features, you can set <code>show_pred=\"true\"</code> and provide several sentences with <code>pred_texts</code> argument. The value of <code>pred_texts</code> should be a list of strings. The probability that each frame corresponds to all the sentences you provide will be output. <pre><code>python main.py \\\n    feature_type=\"clip\" \\\n    model_name=\"ViT-B/32\" \\\n    device=\"cuda:0\" \\\n    extraction_fps=1 \\\n    show_pred=\"true\" \\\n    pred_texts=\"['a dog smiles', 'a woman is lifting']\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> You will get the output for each frame like: <pre><code>  Logits | Prob. | Label\n  23.061 | 0.962 | a dog smiles\n  19.824 | 0.038 | a woman is lifting\n\n  Logits | Prob. | Label\n  22.770 | 0.963 | a dog smiles\n  19.520 | 0.037 | a woman is lifting\n\n  Logits | Prob. | Label\n  24.619 | 0.929 | a dog smiles\n  22.048 | 0.071 | a woman is lifting\n...\n\n  Logits | Prob. | Label\n  30.966 | 1.000 | a woman is lifting\n  15.272 | 0.000 | a dog smiles\n\n  Logits | Prob. | Label\n  32.671 | 1.000 | a woman is lifting\n  15.413 | 0.000 | a dog smiles\n\n  Logits | Prob. | Label\n  32.555 | 1.000 | a woman is lifting\n  16.151 | 0.000 | a dog smiles\n...\n</code></pre></p> <p>You may also leave <code>pred_texts</code> unspecified or <code>null</code> (None) if you wish to apply CLIP for zero-shot prediction on Kinetics 400.</p>"},{"location":"models/clip/#credits","title":"Credits","text":"<ol> <li>The OpenAI CLIP implementation.</li> <li>The CLIP paper</li> <li>Thanks to @Kamino666 who adapted this model for <code>video_features</code></li> </ol>"},{"location":"models/clip/#license","title":"License","text":"<p>The OpenAI CLIP implementation code is under MIT.</p>"},{"location":"models/i3d/","title":"I3D (RGB + Flow)","text":"<p>PWC-Net is deprecated</p> <p>The default behavior has changed in the recent version. Now, the optical flow is extracted with RAFT instead of PWC-Net (deprecated).</p> <p>The Inflated 3D (I3D) features are extracted using a pre-trained model on Kinetics 400. Here, the features are extracted from the second-to-the-last layer of I3D, before summing them up. Therefore, it outputs two tensors with 1024-d features: for RGB and flow streams. By default, it expects to input 64 RGB and flow frames (<code>224x224</code>) which spans 2.56 seconds of the video recorded at 25 fps. In the default case, the features will be of size <code>Tv x 1024</code> where <code>Tv = duration / 2.56</code>.</p> <p>Please note, this implementation uses RAFT optical flow extraction instead of the TV-L1 algorithm, which was used in the original I3D paper as TV-L1 hampers the speed. Yet, it might possibly lead to worse peformance. Our tests show that the performance is reasonable. You may check if the predicted distribution satisfies your requirements for an application. To get the predictions that were made by the classification head, providing the <code>--show_pred</code> flag.</p>"},{"location":"models/i3d/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>stack_size</code> <code>64</code> The number of frames from which to extract features (or window size). <code>step_size</code> <code>64</code> The number of frames to step before extracting the next features. <code>streams</code> <code>null</code> I3D is a two-stream network. By default (<code>null</code> or omitted) both RGB and flow streams are used. To use RGB- or flow-only models use <code>rgb</code> or <code>flow</code>. <code>flow_type</code> <code>raft</code> By default, the flow-features of I3D will be calculated using optical from calculated with RAFT (originally with TV-L1). <code>extraction_fps</code> <code>null</code> If specified (e.g. as <code>5</code>), the video will be re-encoded to the <code>extraction_fps</code> fps. Leave unspecified or <code>null</code> to skip re-encoding. <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos). <code>show_pred</code> <code>false</code> If <code>true</code>, the script will print the predictions of the model on a down-stream task. It is useful for debugging."},{"location":"models/i3d/#quick-start","title":"Quick Start","text":"<p>Ensure that the environment is properly set up before proceeding. See Setup Environment for detailed instructions.</p> <p>Activate the environment <pre><code>conda activate video_features\n</code></pre></p> <p>and extract features from <code>./sample/v_GGSY1Qvo990.mp4</code> video and show the predicted classes <pre><code>python main.py \\\n    feature_type=i3d \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\\n    show_pred=true\n</code></pre></p>"},{"location":"models/i3d/#examples","title":"Examples","text":"<p>Make sure the environment is set up correctly. For instructions, refer to Setup Environment.</p> <p>Activate the environment <pre><code>conda activate video_features\n</code></pre></p> <p>The following will extract I3D features for sample videos. The features are going to be extracted with the default parameters. <pre><code>python main.py \\\n    feature_type=i3d \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p> <p>The video paths can be specified as a <code>.txt</code> file with paths <pre><code>python main.py \\\n    feature_type=i3d \\\n    device=\"cuda:0\" \\\n    file_with_video_paths=./sample/sample_video_paths.txt\n</code></pre> It is also possible to extract features from either <code>rgb</code> or <code>flow</code> modalities individually (<code>--streams</code>) and, therefore, increasing the speed <pre><code>python main.py \\\n    feature_type=i3d \\\n    streams=flow \\\n    device=\"cuda:0\" \\\n    file_with_video_paths=./sample/sample_video_paths.txt\n</code></pre></p> <p>The features can be saved as numpy arrays by specifying <code>--on_extraction save_numpy</code> or <code>save_pickle</code>. By default, it will create a folder <code>./output</code> and will store features there <pre><code>python main.py \\\n    feature_type=i3d \\\n    device=\"cuda:0\" \\\n    on_extraction=save_numpy \\\n    file_with_video_paths=./sample/sample_video_paths.txt\n</code></pre> You can change the output folder using <code>--output_path</code> argument.</p> <p>Also, you may want to try to change I3D window and step sizes <pre><code>python main.py \\\n    feature_type=i3d \\\n    device=\"cuda:0\" \\\n    stack_size=24 \\\n    step_size=24 \\\n    file_with_video_paths=./sample/sample_video_paths.txt\n</code></pre></p> <p>By default, the frames are extracted according to the original fps of a video. If you would like to extract frames at a certain fps, specify <code>--extraction_fps</code> argument. <pre><code>python main.py \\\n    feature_type=i3d \\\n    device=\"cuda:0\" \\\n    extraction_fps=25 \\\n    stack_size=24 \\\n    step_size=24 \\\n    file_with_video_paths=./sample/sample_video_paths.txt\n</code></pre> A fun note, the time span of the I3D features in the last example will match the time span of VGGish features with default parameters (24/25 = 0.96).</p> <p>If <code>--keep_tmp_files</code> is specified, it keeps them in <code>--tmp_path</code> which is <code>./tmp</code> by default. Be careful with the <code>--keep_tmp_files</code> argument when playing with <code>--extraction_fps</code> as it may mess up the frames you extracted before in the same folder.</p>"},{"location":"models/i3d/#credits","title":"Credits","text":"<ol> <li>The Official RAFT implementation (esp. <code>./demo.py</code>).</li> <li>A port of I3D weights from TensorFlow to PyTorch</li> <li>The I3D paper: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset.</li> </ol>"},{"location":"models/i3d/#license","title":"License","text":"<p>The wrapping code is MIT and the port of I3D weights from TensorFlow to PyTorch. RAFT BSD 3-Clause.</p>"},{"location":"models/pwc/","title":"PWC-Net","text":"<p>The PWC-New is no longer supported</p> <p>Thank you for your interest in this model. However, I am no longer supporting it. The reason is that: a) it requires a separate conda environment, b) it depends on <code>cupy</code> that is difficult to maintain, c) current implementation does not support newer GPUs, d) RAFT is a better alternative, e) the implementation that I used is licensed as GPL which makes the whole library to be GPL. Overall, it is not worth the effort to maintain it. To use it, please refer to the latest commit that supports it: <code>bd827df</code>.</p> <p>PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume frames are extracted for every consecutive pair of frames in a video. PWC-Net is pre-trained on Sintel Flow dataset. The implementation follows sniklaus/pytorch-pwc@f61389005.</p> <p>CUDA 11 and GPUs like RTX 3090 and newer</p> <p>The current environment does not support CUDA 11 and, therefore, GPUs like RTX 3090 and newer. For details please check this issue #13 If you were able to fix it, please share your workaround. If you need an optical flow extractor, you are recommended to use RAFT.</p> <p>The PWC-Net does NOT support using CPU currently</p> <p>The PWC-Net uses <code>cupy</code> module, which makes it difficult to turn to a version that does not use the GPU. However, if you have solution, you may submit a PR.</p>"},{"location":"models/pwc/#set-up-the-environment-for-pwc","title":"Set up the Environment for PWC","text":"<p>Setup <code>conda</code> environment. <pre><code># it will create a new conda environment called 'pwc' on your machine\nconda env create -f conda_env_pwc.yml\n</code></pre></p>"},{"location":"models/pwc/#quick-start","title":"Quick Start","text":"<p>Activate the environment <pre><code>conda activate pwc\n</code></pre></p> <p>and extract optical flow from <code>./sample/v_GGSY1Qvo990.mp4</code> and show the flow for each frame <pre><code>python main.py \\\n    feature_type=pwc \\\n    device=\"cuda:0\" \\\n    show_pred=true \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> Note, if <code>show_pred=true</code>, the window with predictions will appear, use any key to show the next frame. To use <code>show_pred=true</code>, a screen must be attached to the machine or X11 forwarding is enabled.</p>"},{"location":"models/pwc/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>batch_size</code> <code>1</code> You may speed up extraction of features by increasing the batch size as much as your GPU permits. <code>extraction_fps</code> <code>null</code> If specified (e.g. as <code>5</code>), the video will be re-encoded to the <code>extraction_fps</code> fps. Leave unspecified or <code>null</code> to skip re-encoding. <code>side_size</code> <code>null</code> If resized to the smaller edge (<code>resize_to_smaller_edge=true</code>), then <code>min(W, H) = side_size</code>, if to the larger: max(W, H), if <code>null</code> (None) no resize is performed. <code>resize_to_smaller_edge</code> <code>true</code> If <code>false</code>, the larger edge will be used to be resized to <code>side_size</code>. <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos). <code>show_pred</code> <code>false</code> If <code>true</code>, the script will visualize the optical flow for each pair of RGB frames."},{"location":"models/pwc/#examples","title":"Examples","text":"<p>Please see the examples for <code>RAFT</code> optical flow frame extraction. Make sure to replace <code>--feature_type</code> argument to <code>pwc</code>.</p>"},{"location":"models/pwc/#credits","title":"Credits","text":"<ol> <li>The PWC-Net paper and official implementation.</li> <li>The PyTorch implementation used in this repo.</li> </ol>"},{"location":"models/pwc/#license","title":"License","text":"<p>The wrapping code is under MIT, but PWC Net has GPL-3.0</p>"},{"location":"models/r21d/","title":"R(2+1)D","text":"<p>We support 3 flavors of R(2+1)D:</p> <ul> <li><code>r2plus1d_18_16_kinetics</code> 18-layer R(2+1)D pre-trained on Kinetics 400 (used by default) \u2013 it is identical to the torchvision implementation</li> <li><code>r2plus1d_34_32_ig65m_ft_kinetics</code> 34-layer R(2+1)D pre-trained on IG-65M and fine-tuned on Kinetics 400 \u2013 the weights are provided by moabitcoin/ig65m-pytorch repo for stack/step size <code>32</code>.</li> <li><code>r2plus1d_34_8_ig65m_ft_kinetics</code> the same as the one above but this one was pre-trained with stack/step size <code>8</code></li> </ul> <p>models are pre-trained on RGB frames and follow the plain torchvision augmentation sequence.</p> <p>Info</p> <p>The flavors that were pre-trained on IG-65M and fine-tuned on Kinetics 400 yield significantly better performance than the default model (e.g. the <code>32</code> frame model reaches an accuracy of 79.10 vs 57.50 by default).</p> <p>By default (<code>model_name=r2plus1d_18_16_kinetics</code>), the model expects to input a stack of 16 RGB frames (<code>112x112</code>), which spans 0.64 seconds of the video recorded at 25 fps. In the default case, the features will be of size <code>Tv x 512</code> where <code>Tv = duration / 0.64</code>. Specify, <code>model_name</code>, <code>step_size</code> and <code>stack_size</code> to change the default behavior.</p>"},{"location":"models/r21d/#quick-start","title":"Quick Start","text":"<p>Ensure that the environment is properly set up before proceeding. See Setup Environment for detailed instructions.</p> <p>Activate the environment <pre><code>conda activate video_features\n</code></pre></p> <p>and extract features from the <code>./sample/v_GGSY1Qvo990.mp4</code> video and show the predicted classes <pre><code>python main.py \\\n    feature_type=r21d \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\\n    show_pred=true\n</code></pre></p>"},{"location":"models/r21d/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>model_name</code> <code>\"r2plus1d_18_16_kinetics\"</code> A variant of R(2+1)d.  <code>\"r2plus1d_18_16_kinetics\"</code>, <code>\"r2plus1d_34_32_ig65m_ft_kinetics\"</code>, <code>\"r2plus1d_34_8_ig65m_ft_kinetics\"</code> are supported. <code>stack_size</code> <code>null</code> The number of frames from which to extract features (or window size). If omitted, it will respect the config of <code>model_name</code> during training. <code>step_size</code> <code>null</code> The number of frames to step before extracting the next features. If omitted, it will respect the config of <code>model_name</code> during training. <code>extraction_fps</code> <code>null</code> If specified (e.g. as <code>5</code>), the video will be re-encoded to the <code>extraction_fps</code> fps. Leave unspecified or <code>null</code> to skip re-encoding. <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos). <code>show_pred</code> <code>false</code> If <code>true</code>, the script will print the predictions of the model on a down-stream task. It is useful for debugging."},{"location":"models/r21d/#example","title":"Example","text":"<p>Make sure the environment is set up correctly. For instructions, refer to Setup Environment.</p> <p>Start by activating the environment <pre><code>conda activate video_features\n</code></pre></p> <p>It will extract R(2+1)d features for two sample videos. The features are going to be extracted with the default parameters. <pre><code>python main.py \\\n    feature_type=r21d \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p> <p>Here is an example with <code>r2plus1d_34_32_ig65m_ft_kinetics</code> 34-layer R(2+1)D model that waas pre-trained on IG-65M and, then, fine-tuned on Kinetics 400 <pre><code>python main.py \\\n    feature_type=r21d \\\n    model_name=\"r2plus1d_34_8_ig65m_ft_kinetics\" \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p> <p>See the config file for other supported parameters. Note, that this implementation of R(2+1)d only supports the RGB stream.</p>"},{"location":"models/r21d/#credits","title":"Credits","text":"<ol> <li>The TorchVision implementation.</li> <li>The R(2+1)D paper: A Closer Look at Spatiotemporal Convolutions for Action Recognition.</li> <li>Thanks to @ohjho we now also support the favors of the 34-layer model pre-trained on IG-65M and fine-tuned on Kinetics 400.<ul> <li>A shout-out to devs of moabitcoin/ig65m-pytorch who adapted weights of these favors from Caffe to PyTorch.</li> <li>The paper where these flavors were presented:  Large-scale weakly-supervised pre-training for video action recognition</li> </ul> </li> </ol>"},{"location":"models/r21d/#license","title":"License","text":"<p>The wrapping code is under MIT, yet, it utilizes <code>torchvision</code> library which is under BSD 3-Clause \"New\" or \"Revised\" License.</p>"},{"location":"models/raft/","title":"RAFT","text":"<p>Recurrent All-Pairs Field Transforms for Optical Flow (RAFT) frames are extracted for every consecutive pair of frames in a video. The implementation follows the official implementation. RAFT is pre-trained on FlyingChairs, fine-tuned on FlyingThings3D, then it is finetuned on Sintel or KITTI-2015 (see the Training Schedule in the Experiments section in the RAFT paper). Also, check out and this issue to learn more about the shared models.</p> <p>The optical flow frames have the same size as the video input or as specified by the resize arguments. We additionally output timesteps in ms for each feature and fps of the video.</p>"},{"location":"models/raft/#quick-start","title":"Quick Start","text":"<p>Ensure that the environment is properly set up before proceeding. See Setup Environment for detailed instructions.</p> <p>Activate the environment <pre><code>conda activate video_features\n</code></pre></p> <p>and extract optical flow from <code>./sample/v_GGSY1Qvo990.mp4</code> using one GPU and show the flow for each frame <pre><code>python main.py \\\n    feature_type=raft \\\n    show_pred=true \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> Note, if <code>show_pred=true</code>, the window with predictions will appear, use any key to show the next frame. To use <code>show_pred=true</code>, a screen must be attached to the machine or X11 forwarding is enabled.</p>"},{"location":"models/raft/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>finetuned_on</code> <code>sintel</code> The RAFT model is pre-trained on FlyingChairs, then it is fine-tuned on FlyingThings3D, and then it is fine-tuned on <code>finetuned_on</code> dataset that can be either <code>sintel</code> or <code>kitti</code>. <code>batch_size</code> <code>1</code> You may speed up extraction of features by increasing the batch size as much as your GPU permits. <code>extraction_fps</code> <code>null</code> If specified (e.g. as <code>5</code>), the video will be re-encoded to the <code>extraction_fps</code> fps. Leave unspecified or <code>null</code> to skip re-encoding. <code>side_size</code> <code>null</code> If resized to the smaller edge (<code>resize_to_smaller_edge=true</code>), then <code>min(W, H) = side_size</code>, if to the larger: max(W, H), if <code>null</code> (None) no resize is performed. <code>resize_to_smaller_edge</code> <code>true</code> If <code>false</code>, the larger edge will be used to be resized to <code>side_size</code>. <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos). <code>show_pred</code> <code>false</code> If <code>true</code>, the script will visualize the optical flow for each pair of RGB frames."},{"location":"models/raft/#examples","title":"Examples","text":"<p>Make sure the environment is set up correctly. For instructions, refer to Setup Environment.</p> <p>Start by activating the environment <pre><code>conda activate video_features\n</code></pre></p> <p>A minimal working example: it will extract RAFT optical flow frames for sample videos. <pre><code>python main.py \\\n    feature_type=raft \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> Note, if your videos are quite long, have large dimensions and fps, watch your RAM as the frames are stored in the memory until they are saved. Please see other examples how can you overcome this problem.</p> <p>By default, the frames are extracted using the Sintel model. If you wish you can use KITTI-pretrained model by changing the <code>finetuned_on</code> argument: <pre><code>python main.py \\\n    feature_type=raft \\\n    device=\"cuda:0\" \\\n    finetuned_on=kitti \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p> <p>If you would like to save the frames, use <code>--on_extraction save_numpy</code> (or <code>save_pickle</code>) \u2013 by default, the frames are saved in <code>./output/</code> or where <code>--output_path</code> specifies. In the case of RAFT, besides frames, it also saves timestamps in ms and the original fps of the video into the same folder with features. <pre><code>python main.py \\\n    feature_type=raft \\\n    device=\"cuda:0\" \\\n    on_extraction=save_numpy \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> Since extracting flow between two frames is cheap we may increase the extraction speed with batching. Therefore, you can use <code>--batch_size</code> argument (defaults to <code>1</code>) to do so. A precaution: make sure to properly test the memory impact of using a specific batch size if you are not sure which kind of videos you have. For instance, you tested the extraction on 16:9 aspect ratio videos but some videos are 16:10 which might give you a mem error. Therefore, I would recommend to tune <code>--batch_size</code> on a square video and using the resize arguments (showed later) <pre><code>python main.py \\\n    feature_type=raft \\\n    device=\"cuda:0\" \\\n    batch_size=16 \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> Another way of speeding up the extraction is to resize the input frames. Use <code>resize_to_smaller_edge=true</code> (default) if you would like <code>--side_size</code> to be <code>min(W, H)</code> if <code>resize_to_smaller_edge=false</code> the <code>--side_size</code> value will correspond to be <code>max(W, H)</code> . The latter might be useful when you are not sure which aspect ratio the videos have (the upper bound on size). <pre><code>python main.py \\\n    feature_type=raft \\\n    device=\"cuda:0\" \\\n    side_size=256 \\\n    resize_to_smaller_edge=false \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> If the videos have different fps rate, <code>--extraction_fps</code> might be used to specify the target fps of all videos (a video is reencoded and saved to <code>--tmp_path</code> folder and deleted if <code>--keep_tmp_files</code> wasn't used). <pre><code>python main.py \\\n    feature_type=raft \\\n    device=\"cuda:0\" \\\n    extraction_fps=1 \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p>"},{"location":"models/raft/#credits","title":"Credits","text":"<ol> <li>The Official RAFT implementation (esp. <code>./demo.py</code>).</li> <li>The RAFT paper: RAFT: Recurrent All Pairs Field Transforms for Optical Flow.</li> </ol>"},{"location":"models/raft/#license","title":"License","text":"<p>The wrapping code is under MIT, but the RAFT implementation complies with BSD 3-Clause.</p>"},{"location":"models/resnet/","title":"ResNet","text":"<p>The ResNet features are extracted at each frame of the provided video. The ResNet is pre-trained on the 1k ImageNet dataset. We extract features from the pre-classification layer. The implementation is based on the torchvision models. The extracted features are going to be of size <code>num_frames x 2048</code>. We additionally output timesteps in ms for each feature and fps of the video. We use the standard set of augmentations.</p>"},{"location":"models/resnet/#quick-start","title":"Quick Start","text":"<p>Ensure that the environment is properly set up before proceeding. See Setup Environment for detailed instructions.</p> <p>Activate the environment <pre><code>conda activate video_features\n</code></pre></p> <p>and extract features at 1 fps from <code>./sample/v_GGSY1Qvo990.mp4</code> video and show the predicted classes <pre><code>python main.py \\\n    feature_type=resnet \\\n    model_name=resnet101 \\\n    extraction_fps=1 \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\\n    show_pred=true\n</code></pre></p>"},{"location":"models/resnet/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>model_name</code> <code>resnet50</code> A variant of ResNet. <code>resnet18</code> <code>resnet34</code> <code>resnet50</code> <code>resnet101</code> <code>resnet151</code> are supported. <code>batch_size</code> <code>1</code> You may speed up extraction of features by increasing the batch size as much as your GPU permits. <code>extraction_fps</code> <code>null</code> If specified (e.g. as <code>5</code>), the video will be re-encoded to the <code>extraction_fps</code> fps. Leave unspecified or <code>null</code> to skip re-encoding. <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos). <code>show_pred</code> <code>false</code> If <code>true</code>, the script will print the predictions of the model on a down-stream task. It is useful for debugging."},{"location":"models/resnet/#examples","title":"Examples","text":"<p>Make sure the environment is set up correctly. For instructions, refer to Setup Environment.</p> <p>Start by activating the environment <pre><code>conda activate video_features\n</code></pre></p> <p>It is pretty much the same procedure as with other features. The example is provided for the ResNet-50 flavour, but we also support ResNet-18,34,101,152. You can specify the model with the <code>model_name</code> parameter <pre><code>python main.py \\\n    feature_type=resnet \\\n    model_name=resnet50 \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> If you would like to save the features, use <code>--on_extraction save_numpy</code> (or <code>save_pickle</code>) \u2013 by default, the features are saved in <code>./output/</code> or where <code>--output_path</code> specifies. In the case of frame-wise features, besides features, it also saves timestamps in ms and the original fps of the video into the same folder with features. <pre><code>python main.py \\\n    feature_type=resnet \\\n    model_name=resnet50 \\\n    device=\"cuda:0\" \\\n    on_extraction=save_numpy \\\n    file_with_video_paths=./sample/sample_video_paths.txt\n</code></pre> Since these features are so fine-grained and light-weight we may increase the extraction speed with batching. Therefore, frame-wise features have <code>--batch_size</code> argument, which defaults to <code>1</code>. <pre><code>python main.py \\\n    feature_type=resnet \\\n    model_name=resnet50 \\\n    device=\"cuda:0\" \\\n    batch_size=128 \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> If you would like to extract features at a certain fps, add <code>--extraction_fps</code> argument <pre><code>python main.py \\\n    feature_type=resnet \\\n    model_name=resnet50 \\\n    device=\"cuda:0\" \\\n    extraction_fps=5 \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p>"},{"location":"models/resnet/#credits","title":"Credits","text":"<ol> <li>The TorchVision implementation.</li> <li>The ResNet paper</li> </ol>"},{"location":"models/resnet/#license","title":"License","text":"<p>The wrapping code is under MIT, yet, it utilizes <code>torchvision</code> library which is under BSD 3-Clause \"New\" or \"Revised\" License.</p>"},{"location":"models/s3d/","title":"S3D","text":"<p>The S3D action recognition model was originally introduced in Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification. We support the PyTorch weights for Kinetics 400 provided by github.com/kylemin/S3D. According to the model card, with these weights, the model should achieve 72.08% top-1 accuracy (top5: 90.35%) on the Kinetics 400 validation set.</p> <p>How the model was pre-trained? My best educated guess is that the model was trained on densely sampled 64-frame <code>224 x 224</code> stacks that were randomly trimmed and cropped from 25 fps <code>256 x 256</code> video clips (&lt;= 10 sec). Therefore, to extract features (<code>Tv x 1024</code>), we resize the input video such that <code>min(H, W) = 224</code> (?) and take the center crop to make it <code>224 x 224</code>. By default, the feature extractor will split the input video into 64-stack frames (2.56 sec) with no overlap as it is during the pre-training and will do a forward pass on each of them. This should be similar to I3D behavior. For instance, given an ~18-second 25 fps video, the features will be of size <code>7 x 1024</code>. Specify, <code>step_size</code>, <code>extraction_fps</code>, and <code>stack_size</code> to change the default behavior.</p> <p>What is extracted exactly? The inputs to the classification head (see <code>S3D.fc</code> and <code>S3D.forward</code>) that were average-pooled across the time dimension.</p>"},{"location":"models/s3d/#quick-start","title":"Quick Start","text":"<p>Ensure that the environment is properly set up before proceeding. See Setup Environment for detailed instructions.</p> <p>Activate the environment <pre><code>conda activate video_features\n</code></pre></p> <p>and extract features from the <code>./sample/v_GGSY1Qvo990.mp4</code> video and show the predicted classes <pre><code>python main.py \\\n    feature_type=s3d \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\\n    show_pred=true\n</code></pre></p> <p>See the config file for ther supported parameters.</p>"},{"location":"models/s3d/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>stack_size</code> <code>64</code> The number of frames from which to extract features (or window size). If omitted, it will respect the config of <code>model_name</code> during training. <code>step_size</code> <code>64</code> The number of frames to step before extracting the next features. If omitted, it will respect the config of <code>model_name</code> during training. <code>extraction_fps</code> <code>25</code> If specified (e.g. as <code>5</code>), the video will be re-encoded to the <code>extraction_fps</code> fps. Leave unspecified or <code>null</code> to skip re-encoding. <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos). <code>show_pred</code> <code>false</code> If <code>true</code>, the script will print the predictions of the model on a down-stream task. It is useful for debugging."},{"location":"models/s3d/#credits","title":"Credits","text":"<ol> <li>The kylemin/S3D implementation.</li> <li>The S3D paper: Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification.</li> </ol>"},{"location":"models/s3d/#license","title":"License","text":"<p>MIT</p>"},{"location":"models/timm/","title":"timm","text":"<p><code>video_features</code> \u2764\ufe0f timm. We support all the models from the <code>timm</code> library (technically, for those where you can specify <code>pretrained=True</code>).</p> <p>For details, see the timm docs and, specifically model summaries and model benchmark results.</p>"},{"location":"models/timm/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>model_name</code> <code>null</code> Any model from <code>timm.list_pretrained()</code>, e.g. <code>efficientnet_b0</code> or <code>efficientnet_b0.ra_in1k</code>. <code>batch_size</code> <code>1</code> You may speed up extraction of features by increasing the batch size as much as your GPU permits. <code>extraction_fps</code> <code>null</code> If specified (e.g. as <code>5</code>), the video will be re-encoded to the <code>extraction_fps</code> fps. Leave unspecified or <code>null</code> to skip re-encoding. <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos). <code>show_pred</code> <code>false</code> If <code>true</code>, the script will print the predictions of the model on a down-stream task. It is useful for debugging. This flag is only supported for the models that were trained on ImageNet 1K and 21K."},{"location":"models/timm/#examples","title":"Examples","text":"<pre><code>python main.py \\\n    feature_type=timm \\\n    model_name=efficientnet_b0 \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre> <p>If you want to specify particular weights, you can do it with <code>model_name</code> argument, as you'd do with <code>timm</code>, e.g. <pre><code>python main.py \\\n    feature_type=timm \\\n    model_name=efficientnet_b0.ra_in1k \\\n    device=\"cuda:0\" \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p> <p>If you'd like to check the model's outputs on a downstream task (ImageNet 1K or 21K), you can use <code>show_pred</code> argument. <pre><code>python main.py \\\n    feature_type=timm \\\n    model_name=swin_small_patch4_window7_224.ms_in22k \\\n    device=\"cuda:0\" \\\n    extraction_fps=1 \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\\n    show_pred=true\n#   Logits | Prob. | Label\n#   12.029 | 0.456 | barbell\n#   11.676 | 0.321 | weight, free_weight, exercising_weight\n#    9.653 | 0.042 | pusher, thruster\n#    9.499 | 0.036 | dumbbell\n#    8.787 | 0.018 | bench_press\n\n#   Logits | Prob. | Label\n#   11.742 | 0.467 | barbell\n#   11.233 | 0.281 | weight, free_weight, exercising_weight\n#    9.489 | 0.049 | dumbbell\n#    8.923 | 0.028 | pusher, thruster\n#    8.406 | 0.017 | bench_press\n\n#   Logits | Prob. | Label\n#   12.257 | 0.571 | barbell\n#   11.391 | 0.240 | weight, free_weight, exercising_weight\n#    9.708 | 0.045 | dumbbell\n#    9.031 | 0.023 | pusher, thruster\n#    8.756 | 0.017 | bench_press\n\n#   Logits | Prob. | Label\n#   12.469 | 0.571 | barbell\n#   11.655 | 0.253 | weight, free_weight, exercising_weight\n#    9.818 | 0.040 | dumbbell\n#    9.648 | 0.034 | pusher, thruster\n#    8.527 | 0.011 | bench_press\n\n...\n</code></pre></p>"},{"location":"models/timm/#credits","title":"Credits","text":"<ul> <li>timm library</li> </ul>"},{"location":"models/timm/#license","title":"License","text":"<p><code>video_features</code> is under MIT, the <code>timm</code> is under Apache 2.0.</p>"},{"location":"models/vggish/","title":"VGGish","text":"<p>The VGGish feature extraction relies on the PyTorch implementation by harritaylor built to replicate the procedure provided in the TensorFlow repository. The difference in values between the PyTorch and Tensorflow implementation is negligible (see also # difference in values).</p> <p>The VGGish model was pre-trained on AudioSet. The extracted features are from pre-classification layer after activation. The feature tensor will be 128-d and correspond to 0.96 sec of the original video. Interestingly, this might be represented as 24 frames of a 25 fps video. Therefore, you should expect <code>Ta x 128</code> features, where <code>Ta = duration / 0.96</code>.</p> <p>The extraction of VGGish features is implemeted as a wrapper of the TensorFlow implementation. See Credits.</p>"},{"location":"models/vggish/#quick-start","title":"Quick Start","text":"<p>Ensure that the environment is properly set up before proceeding. See Setup Environment for detailed instructions.</p> <p>Activate the environment <pre><code>conda activate video_features\n</code></pre></p> <p>and extract features from the <code>./sample/v_GGSY1Qvo990.mp4</code> video <pre><code>python main.py \\\n    feature_type=vggish \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p>"},{"location":"models/vggish/#supported-arguments","title":"Supported Arguments","text":"Argument Default Description <code>device</code> <code>\"cuda:0\"</code> The device specification. It follows the PyTorch style. Use <code>\"cuda:3\"</code> for the 4th GPU on the machine or <code>\"cpu\"</code> for CPU-only. <code>video_paths</code> <code>null</code> A list of videos for feature extraction. E.g. <code>\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"</code> or just one path <code>\"./sample/v_GGSY1Qvo990.mp4\"</code>. <code>file_with_video_paths</code> <code>null</code> A path to a text file with video paths (one path per line). Hint: given a folder <code>./dataset</code> with <code>.mp4</code> files one could use: <code>find ./dataset -name \"*mp4\" &gt; ./video_paths.txt</code>. <code>on_extraction</code> <code>print</code> If <code>print</code>, the features are printed to the terminal. If <code>save_numpy</code> or <code>save_pickle</code>, the features are saved to either <code>.npy</code> file or <code>.pkl</code>. <code>output_path</code> <code>\"./output\"</code> A path to a folder for storing the extracted features (if <code>on_extraction</code> is either <code>save_numpy</code> or <code>save_pickle</code>). <code>keep_tmp_files</code> <code>false</code> If <code>true</code>, the reencoded videos will be kept in <code>tmp_path</code>. <code>tmp_path</code> <code>\"./tmp\"</code> A path to a folder for storing temporal files (e.g. reencoded videos)."},{"location":"models/vggish/#example","title":"Example","text":"<p>Make sure the environment is set up correctly. For instructions, refer to Setup Environment.</p> <p>Activate the environment</p> <pre><code>conda activate video_features\n</code></pre> <p>The video paths can be specified as a <code>.txt</code> file with paths. <pre><code>python main.py \\\n    feature_type=vggish \\\n    device=\"cuda:0\" \\\n    file_with_video_paths=./sample/sample_video_paths.txt\n</code></pre> The features can be saved as numpy arrays by specifying <code>--on_extraction save_numpy</code> or <code>save_pickle</code>. By default, it will create a folder <code>./output</code> and will store features there (you can change the output folder using <code>--output_path</code>) <pre><code>python main.py \\\n    feature_type=vggish \\\n    device=\"cuda:0\" \\\n    on_extraction=save_numpy \\\n    video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"\n</code></pre></p>"},{"location":"models/vggish/#difference-between-tensorflow-and-pytorch-implementations","title":"Difference between TensorFlow and PyTorch implementations","text":"<p>VGGish was originally implemented in TensorFlow. We use the PyTorch implementation by harritaylor/torchvggish The difference in values between the PyTorch and Tensorflow implementation is negligible. However, after updating the versions of the dependencies, the values are slightly different. If you wish to use the old implementation, you can use the conda environment at the <code>b21f330</code> commit or earlier. The following table shows the difference in values.</p> <pre><code>python main.py \\\n    feature_type=vggish \\\n    video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\"\n\nOriginal (./sample/v_GGSY1Qvo990.mp4):\n[[0.         0.04247099 0.09079538 ... 0.         0.18485409 0.        ]\n [0.         0.         0.         ... 0.         0.5720243  0.5475726 ]\n [0.         0.00705254 0.15173683 ... 0.         0.33540994 0.10572422]\n ...\n [0.         0.         0.36020872 ... 0.         0.08559107 0.00870359]\n [0.         0.21485361 0.16507196 ... 0.         0.         0.        ]\n [0.         0.31638345 0.         ... 0.         0.         0.        ]]\nmax: 2.31246495; mean: 0.13741589; min: 0.00000000\n\nb21f330 and ealier (./sample/v_GGSY1Qvo990.mp4):\n[[0.         0.04247095 0.09079528 ... 0.         0.18485469 0.        ]\n [0.         0.         0.         ... 0.         0.5720252  0.5475726 ]\n [0.         0.0070536  0.1517372  ... 0.         0.33541012 0.10572463]\n ...\n [0.         0.         0.36020786 ... 0.         0.08559084 0.00870359]\n [0.         0.21485506 0.16507116 ... 0.         0.         0.        ]\n [0.         0.31638315 0.         ... 0.         0.         0.        ]]\nmax: 2.31246495; mean: 0.13741589; min: 0.00000000\n\nCurrent (./sample/v_GGSY1Qvo990.mp4):\n[[0.         0.0752698  0.12985817 ... 0.         0.18340725 0.00647891]\n [0.         0.         0.         ... 0.         0.5479691  0.6105871 ]\n [0.         0.03563304 0.1507446  ... 0.         0.20983526 0.15856776]\n ...\n [0.         0.         0.3077196  ... 0.         0.08271158 0.03223182]\n [0.         0.15476668 0.25240228 ... 0.         0.         0.        ]\n [0.         0.3711498  0.         ... 0.         0.         0.        ]]\nmax: 2.41924119; mean: 0.13830526; min: 0.00000000\n</code></pre>"},{"location":"models/vggish/#credits","title":"Credits","text":"<ol> <li>The PyTorch implementation of vggish.</li> <li>The VGGish paper: CNN Architectures for Large-Scale Audio Classification.</li> </ol>"},{"location":"models/vggish/#license","title":"License","text":"<p>The wrapping code is under MIT but the <code>vggish</code> implementation complies with the <code>harritaylor/torchvggish</code> (same as tensorflow) license which is Apache-2.0.</p>"}]}